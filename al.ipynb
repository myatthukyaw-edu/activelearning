{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "al.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7YVb7C1Tmpt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Flatten, Dense, Conv2D, MaxPool2D, Dropout,MaxPooling2D, Activation\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "from sklearn.utils import check_random_state\n",
        "#from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5q-C3SJSc-r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "def get_dataset():\n",
        "  (X_train_full, y_train_full), (X_test, y_test) = cifar10.load_data()\n",
        "  # summarize loaded dataset\n",
        "  print('Cifar Train: X=%s, y=%s' % (X_train_full.shape, y_train_full.shape))\n",
        "  print('Cifar Test: X=%s, y=%s' % (X_test.shape, y_test.shape))\n",
        "  return (X_train_full, y_train_full, X_test, y_test)\n",
        "\n",
        "class Normalize(object):\n",
        "    \n",
        "    def normalize(self, X, y):\n",
        "        X = X.astype('float32')/255.\n",
        "        y = y.astype('int32')\n",
        "        return (X, y) \n",
        "    \n",
        "    def inverse(self, X, y):\n",
        "        X = X.astype('float32')*255.\n",
        "        y = y.astype('int32')\n",
        "        return (X, y) "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBRe6hO6Sibk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils import check_random_state\n",
        "import numpy as np\n",
        "\n",
        "def get_k_random_samples(trainset_size, initial_labeled_samples, X_train_full,\n",
        "                         y_train_full):\n",
        "    random_state = check_random_state(0)\n",
        "    permutation = np.random.choice(trainset_size, initial_labeled_samples, replace=False)\n",
        "\n",
        "    print ('Initial random chosen samples', permutation.shape)\n",
        "    X_train = X_train_full[permutation]\n",
        "    y_train = y_train_full[permutation]\n",
        "    #X_train = X_train.reshape((X_train.shape[0], -1))\n",
        "    y_train_bin = y_train.reshape(initial_labeled_samples,)\n",
        "    bin_count = np.bincount(y_train_bin.astype('int64'))\n",
        "    unique = np.unique(y_train)\n",
        "    print ( 'initial train set:', X_train.shape, y_train.shape,\n",
        "        '\\nlabels count:', bin_count, unique,\n",
        "        )\n",
        "    return (permutation, X_train, y_train)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJnZmzM4SobB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from scipy.stats import entropy\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "labels= ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "class RandomSelection:\n",
        "    def __init__(self):\n",
        "      pass\n",
        "\n",
        "    #@staticmethod\n",
        "    def select(self, probas_val, initial_labeled_samples):\n",
        "        random_state = check_random_state(0)\n",
        "        selection = np.random.choice(probas_val.shape[0], initial_labeled_samples, replace=False)\n",
        "        return selection\n",
        "\n",
        "class QBC:\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def average_KL_divergence(self, probas_val, probas_val2, X_train, y_train, X_seedset, y_seedset, samples):\n",
        "      preds = []\n",
        "      preds.append(probas_val)\n",
        "      preds.append(probas_val2)\n",
        "\n",
        "      #change normal arry to numpy array with stack\n",
        "      consensus = np.mean(np.stack(preds), axis=0)\n",
        "      print('consensus :',consensus)\n",
        "      divergence = []\n",
        "      for y_out in preds:\n",
        "        divergence.append(entropy(consensus.T, y_out.T))\n",
        "\n",
        "      result = np.apply_along_axis(np.mean, 0, np.stack(divergence))\n",
        "\n",
        "      #argsort Returns the indices that would sort an array.\n",
        "      #argsort sort in increasing order but if use (-result) then it sort in decreasing order.\n",
        "      rankings = np.argsort(-result)[:samples]\n",
        "\n",
        "      return rankings\n",
        "\n",
        "    def vote_entropy(self, probas_val, probas_val2, X_train, y_train, X_seedset, y_seedset, samples):\n",
        "      #vote entropy\n",
        "      preds = []\n",
        "\n",
        "      probas_val_not_cat = np.argmax(probas_val,axis=1)\n",
        "      probas_val2_not_cat = np.argmax(probas_val2, axis=-1)\n",
        "\n",
        "      preds.append(np.eye(len(labels))[probas_val_not_cat])\n",
        "      preds.append(np.eye(len(labels))[probas_val2_not_cat])\n",
        "\n",
        "      # C = no of models\n",
        "      C = 2\n",
        "      votes = np.apply_along_axis(np.sum, 0, np.stack(preds)) / C\n",
        "      results = np.apply_along_axis(entropy, 1, votes)\n",
        "\n",
        "      rankings = np.argsort(-results)[:samples]\n",
        "\n",
        "      return rankings\n",
        "\n",
        "class uncertainty_sampling:\n",
        "    def init(self):\n",
        "      pass\n",
        "\n",
        "    def least_confident(self, probs, samples):\n",
        "        #get the least uncertain value by subtracting the most uncertain value from 100% or 1\n",
        "        scores = 1 - np.amax(probs, axis=1)\n",
        "        #get the index of the least uncertain \n",
        "        rankings = np.argsort(-scores)[:samples]\n",
        "        return rankings\n",
        "    \n",
        "    def max_margin(self, probs, samples):\n",
        "        margin = np.partition(-probs, 1, axis=1)\n",
        "        scores = -np.abs(margin[:,0] - margin[:, 1])\n",
        "        rankings = np.argsort(-scores)[:samples]\n",
        "        return rankings\n",
        "\n",
        "    def entropy(self, probs, samples):\n",
        "        socres = np.apply_along_axis(entropy,1, probs)\n",
        "        rankings = np.argsort(-scores)[:samples]\n",
        "        return rankings"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HroBU0D3Sv27",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4d9161f1-a6e8-4590-b898-a7dc4737d7f6"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Flatten, Dense, Conv2D, MaxPool2D, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "#%load_ext tensorboard\n",
        "\n",
        "def train_model1( X_train_org, y_train_org,X_seedset, X_test, y_test, labels, iterations):\n",
        "  \n",
        "        print(\"\\nModel 1 training\")\n",
        "\n",
        "        X_train, X_val, y_train, y_val = train_test_split( X_train_org, y_train_org, test_size=0.3, random_state=42)\n",
        "        print('X_train :', X_train.shape)\n",
        "        print('y train :', y_train.shape)\n",
        "        print('X val :', X_val.shape)\n",
        "        print('y val:', y_val.shape)\n",
        "\n",
        "        # filepath = 'modelfiles/model_1.h5'\n",
        "        # cp = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "        # log_dir = \"logs/fit/model1\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "        # tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Conv2D(filters=32, kernel_size=(4, 4), input_shape=(32, 32, 3), activation='relu', padding='same'))\n",
        "        model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "        model.add(Dropout(0.25))\n",
        "\n",
        "        model.add(Conv2D(filters=64, kernel_size=(4, 4), activation='relu', padding='same'))\n",
        "        model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "        model.add(Dropout(0.25))\n",
        "\n",
        "        model.add(Conv2D(filters=128, kernel_size=(4, 4), activation='relu', padding='same'))\n",
        "        model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "        model.add(Dropout(0.25))\n",
        "\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(512, activation='relu'))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "        model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "        early_stop = EarlyStopping(monitor='val_loss', patience=2)\n",
        "\n",
        "        r = model.fit(X_train, y_train,  epochs=50, validation_data=(X_val, y_val), verbose=0)#, callbacks=[cp, tensorboard_callback])\n",
        "        \n",
        "        plt.figure(figsize=(12, 8))\n",
        "        plt.subplot(2, 2, 1)\n",
        "        plt.plot(r.history['loss'], label='Loss')\n",
        "        plt.plot(r.history['val_loss'], label='val_Loss')\n",
        "        plt.legend()\n",
        "        plt.title('Loss evolution')\n",
        "        plt.savefig('images/'+str(iterations)+'_model1_loss.png')\n",
        "\n",
        "        plt.subplot(2, 2, 2)\n",
        "        plt.plot(r.history['accuracy'], label='accuracy')\n",
        "        plt.plot(r.history['val_accuracy'], label='val_accuracy')\n",
        "        plt.legend()\n",
        "        plt.title('Accuracy evolution')\n",
        "        plt.savefig('images/'+str(iterations)+'_model1_acc.png')\n",
        "\n",
        "        # Evaluate the model on the test data using `evaluate`\n",
        "        print('\\n# Evaluate on test data')\n",
        "        results = model.evaluate(X_test, y_test, batch_size=128)\n",
        "        print('test loss, test acc:', results)\n",
        "        \n",
        "        test_y_predicted = model.predict(X_test)\n",
        "        test_y_predicted = np.argmax(test_y_predicted, axis=1)\n",
        "        val_y_predicted  = model.predict(X_val)\n",
        "        val_y_predicted = np.argmax(val_y_predicted, axis=1)\n",
        "        cm = confusion_matrix(y_test, test_y_predicted)\n",
        "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "        fig, ax = plt.subplots(figsize=(10, 10))\n",
        "        disp = disp.plot(xticks_rotation='vertical', ax=ax,cmap='summer')\n",
        "        plt.show()\n",
        "        plt.savefig('images/'+str(iterations)+'_model1_cn.png')\n",
        "\n",
        "        probas_val = model.predict(X_seedset)\n",
        "        #print ('probabilities:', probas_val.shape, '\\n', np.argmax(probas_val, axis=1))\n",
        "\n",
        "        recordmodel1.loc[iterations] = [X_train_org.shape[0], X_seedset.shape[0], r.history['accuracy'][-1], r.history['loss'][-1],r.history['val_accuracy'][-1],r.history['val_loss'][1], results[1], results[0]]\n",
        "\n",
        "        return probas_val"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSC7Om8BTL72",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model2( X_train_org, y_train_org,X_seedset, X_test, y_test, labels, iterations):\n",
        "\n",
        "        print(\"\\nModel 2 training\")\n",
        "        \n",
        "        X_train, X_val, y_train, y_val = train_test_split( X_train_org, y_train_org, test_size=0.3, random_state=42)\n",
        "        print('X_train :', X_train.shape)\n",
        "        print('y train :', y_train.shape)\n",
        "        print('X val :', X_val.shape)\n",
        "        print('y val:', y_val.shape)\n",
        "\n",
        "        # filepath = 'modelfiles/model_2.h5'\n",
        "        # cp = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "        # log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "        # tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "        model = tf.keras.Sequential([\n",
        "            \n",
        "            Conv2D(48, kernel_size=(3,3), activation='relu', padding='same', input_shape=(32,32,3)),\n",
        "            MaxPooling2D((2,2),(2,2)),\n",
        "            \n",
        "            Conv2D(96, kernel_size=(3,3), activation='relu', padding='same'),\n",
        "            MaxPooling2D((2,2),(2,2)),\n",
        "            \n",
        "            Conv2D(192,kernel_size=(3,3), activation='relu', padding='same'),\n",
        "            Conv2D(192,kernel_size=(3,3), activation='relu', padding='same'),\n",
        "            MaxPooling2D((2,2),(2,2)),\n",
        "            \n",
        "            Conv2D(256,kernel_size=(3,3), activation='relu', padding='same'),\n",
        "            MaxPooling2D((2,2),(2,2)),\n",
        "\n",
        "            Flatten(),\n",
        "            Dense(512, activation='tanh'),\n",
        "            Dense(256, activation='tanh'),\n",
        "            Dense(10, name='logits'),\n",
        "            Activation('softmax')\n",
        "        ])\n",
        "\n",
        "        model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "        early_stop = EarlyStopping(monitor='val_loss', patience=2)\n",
        "\n",
        "        r = model.fit(X_train, y_train,  epochs=50, validation_data=(X_val, y_val), verbose=0)#, callbacks=[cp, tensorboard_callback])\n",
        "\n",
        "        plt.figure(figsize=(12, 8))\n",
        "\n",
        "        plt.subplot(2, 2, 1)\n",
        "        plt.plot(r.history['loss'], label='Loss')\n",
        "        plt.plot(r.history['val_loss'], label='val_Loss')\n",
        "        plt.legend()\n",
        "        plt.title('Loss evolution')\n",
        "        plt.savefig('images/'+str(iterations)+'_model2_loss.png')\n",
        "\n",
        "        plt.subplot(2, 2, 2)\n",
        "        plt.plot(r.history['accuracy'], label='accuracy')\n",
        "        plt.plot(r.history['val_accuracy'], label='val_accuracy')\n",
        "        plt.legend()\n",
        "        plt.title('Accuracy evolution')\n",
        "        plt.savefig('images/'+str(iterations)+'_model2_acc.png')\n",
        "\n",
        "        # Evaluate the model on the test data using `evaluate`\n",
        "        print('\\n# Evaluate on test data')\n",
        "        results = model.evaluate(X_test, y_test, batch_size=128)\n",
        "        print('test loss, test acc:', results)\n",
        "        \n",
        "        test_y_predicted = model.predict(X_test)\n",
        "        test_y_predicted = np.argmax(test_y_predicted, axis=1)\n",
        "        val_y_predicted  = model.predict(X_val)\n",
        "        val_y_predicted = np.argmax(val_y_predicted, axis=1)\n",
        "        cm = confusion_matrix(y_test, test_y_predicted)\n",
        "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "        fig, ax = plt.subplots(figsize=(10, 10))\n",
        "        disp = disp.plot(xticks_rotation='vertical', ax=ax,cmap='summer')\n",
        "        plt.show()\n",
        "        plt.savefig('images/'+str(iterations)+'model2_cn.png')\n",
        "\n",
        "        probas_val = model.predict(X_seedset)\n",
        "        #print ('probabilities:', probas_val.shape, '\\n', np.argmax(probas_val, axis=1))\n",
        "\n",
        "        recordmodel2.loc[iterations] = [X_train_org.shape[0], X_seedset.shape[0], r.history['accuracy'][-1], r.history['loss'][-1],r.history['val_accuracy'][-1],r.history['val_loss'][1], results[1], results[0]]\n",
        "\n",
        "        return probas_val"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fi0yWkT5TUUt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_fun(X, y):\n",
        "  #X_train, X_test = X_train.astype('int32')*255. , X_test.astype('int8')*255.\n",
        "\n",
        "  # Define the labels of the dataset\n",
        "  labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
        "            'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "  # Let's view more images in a grid format\n",
        "  # Define the dimensions of the plot grid \n",
        "  W_grid = 10\n",
        "  L_grid = 10\n",
        "\n",
        "  # fig, axes = plt.subplots(L_grid, W_grid)\n",
        "  # subplot return the figure object and axes object\n",
        "  # we can use the axes object to plot specific figures at various locations\n",
        "\n",
        "  fig, axes = plt.subplots(L_grid, W_grid, figsize = (17,17))\n",
        "\n",
        "  axes = axes.ravel() # flaten the 15 x 15 matrix into 225 array\n",
        "\n",
        "  n_train = len(X) # get the length of the train dataset\n",
        "\n",
        "  # Select a random number from 0 to n_train\n",
        "  for i in np.arange(0, W_grid * L_grid): # create evenly spaces variables \n",
        "\n",
        "      # Select a random number\n",
        "      index = np.random.randint(0, n_train)\n",
        "      # read and display an image with the selected index    \n",
        "      axes[i].imshow(X[index,1:])\n",
        "      label_index = int(y[index])\n",
        "      axes[i].set_title(labels[label_index], fontsize = 8)\n",
        "      axes[i].axis('off')\n",
        "\n",
        "  plt.subplots_adjust(hspace=0.4)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9l2fHZoTjrZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "4def07fc-b9c1-4334-ed9e-d3cea221b4c5"
      },
      "source": [
        "labels= ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "#define no of samples (to select from pool or seed set) per round\n",
        "samples = 2000\n",
        "trainset_size = 50000\n",
        "\n",
        "#store the accuray and losses in a dataframe\n",
        "recordmodel1 = pd.DataFrame(columns=('TrainDS','Seedset', 'Train_Accuracy', 'Train_Loss', 'Val_Accuracy', 'Val_Loss', 'Test_Accuracy', 'Test_Loss' ))\n",
        "recordmodel2 = pd.DataFrame(columns=('TrainDS','Seedset', 'Train_Accuracy', 'Train_Loss', 'Val_Accuracy', 'Val_Loss', 'Test_Accuracy', 'Test_Loss' ))\n",
        "\n",
        "#get cifar dataset\n",
        "X_train_full, y_train_full, X_test, y_test = get_dataset()\n",
        "\n",
        "#get initial 2000 samples\n",
        "permutation, X_train, y_train = get_k_random_samples(X_train_full.shape[0],samples, X_train_full, y_train_full)\n",
        "\n",
        "print(\"Train set size X :\",X_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "#deine the seedset or pool\n",
        "X_seedset = np.array([])\n",
        "y_seedset = np.array([])\n",
        "X_seedset = np.copy(X_train_full)\n",
        "X_seedset = np.delete(X_seedset, permutation, axis=0)\n",
        "y_seedset = np.copy(y_train_full)\n",
        "y_seedset = np.delete(y_seedset, permutation, axis=0)\n",
        "print ('Seed set (Pool) :', X_seedset.shape, y_seedset.shape)\n",
        "\n",
        "#Normalize\n",
        "normalizer = Normalize()\n",
        "X_train, y_train = normalizer.normalize( X_train, y_train)\n",
        "X_test, y_test = normalizer.normalize(X_test, y_test)\n",
        "X_seedset, y_seedset = normalizer.normalize(X_seedset, y_seedset)\n",
        "\n",
        "#train model\n",
        "#if u r using QBC approach, use both models.\n",
        "#if  r using uncertainty sampling, use only one model.\n",
        "iterations = 0\n",
        "probas_val1 = train_model1(X_train, y_train, X_seedset,X_test, y_test, labels, iterations)\n",
        "probas_val2 = train_model2(X_train, y_train, X_seedset, X_test, y_test, labels, iterations)\n",
        "\n",
        "#choose uncertain samples\n",
        "#QBC\n",
        "#qbc = QBC()\n",
        "#selection_ranking = qbc.vote_entropy(probas_val1, probas_val2,X_train, y_train, X_seedset, y_seedset, samples)\n",
        "\n",
        "#uncertainty sampling \n",
        "us = uncertainty_sampling()\n",
        "#pass the predict values of the desired model as first parameter \n",
        "selection_ranking = us.least_confident(probas_val1, samples)\n",
        "\n",
        "#random selection\n",
        "#rs = RandomSelection()\n",
        "#random_selection = rs.select(probas_val, samples)\n",
        "\n",
        "#get how many uncertain samples are selected from which classes\n",
        "selected_samples = y_seedset[selection_ranking]\n",
        "selected_samples = selected_samples.reshape((selected_samples.shape[0],))\n",
        "bin_count = np.bincount(selected_samples)\n",
        "unique = np.unique(selected_samples)\n",
        "print ('Selected Uncertainty samples :', bin_count, \"from each classes \",unique )\n",
        "\n",
        "#add the selected samples to the training set\n",
        "X_train = np.concatenate((X_train, X_seedset[selection_ranking, :]))\n",
        "y_train = np.concatenate((y_train, y_seedset[selection_ranking]))\n",
        "\n",
        "#delete selected sampling from the seeed set or pool\n",
        "X_seedset = np.delete(X_seedset, selection_ranking, axis=0)\n",
        "y_seedset = np.delete(y_seedset, selection_ranking, axis=0)\n",
        "\n",
        "print('\\nAfter selecting samples based on Samping methods on round',iterations+1,' :')\n",
        "print('Train : ', X_train.shape, y_train.shape)\n",
        "print('Seedset  : ', X_seedset.shape, y_seedset.shape)\n",
        "\n",
        "#print the total number of samples in each class in training set\n",
        "y_train_bin = y_train.reshape((y_train.shape[0],))\n",
        "bin_count = np.bincount(y_train_bin.astype('int64'))\n",
        "unique = np.unique(y_train_bin.astype('int64'))\n",
        "print ('Total number of samples :', bin_count, ' in each classes',unique )\n",
        "\n",
        "iterations = 1\n",
        "while len(X_seedset) > 1 :\n",
        "    print('\\n-------Round ',iterations+1,'----------------')\n",
        "    #normalize\n",
        "    #   normalizer = Normalize()\n",
        "    #   X_train, y_train = normalizer.normalize( X_train, y_train)\n",
        "    #   X_test, y_test = normalizer.normalize(X_test, y_test)    \n",
        "    y_train_bin = y_train.reshape((y_train.shape[0],))\n",
        "    bin_count = np.bincount(y_train_bin.astype('int64'))\n",
        "    unique = np.unique(y_train_bin.astype('int64'))\n",
        "    print ('Total number of samples :', bin_count, ' in each classes',unique )  \n",
        "    #   X_seedset, y_seedset = normalizer.normalize(X_seedset, y_seedset)\n",
        "\n",
        "    #train\n",
        "    probas_val1 = train_model1(X_train, y_train, X_seedset,X_test, y_test, labels, iterations)\n",
        "    probas_val2 = train_model2(X_train, y_train, X_seedset, X_test, y_test, labels, iterations)\n",
        "\n",
        "\n",
        "    #get uncertain examples\n",
        "    selection_ranking = us.least_confident(probas_val1, samples)\n",
        "\n",
        "    # normalization needs to be inversed and recalculated based on the new train and test set.\n",
        "    #   X_train, y_train = normalizer.inverse(X_train, y_train)\n",
        "    #   X_test, y_test = normalizer.inverse(X_test, y_test)\n",
        "    #   X_seedset, y_seedset = normalizer.inverse(X_seedset, y_seedset)\n",
        "    X_train = np.concatenate((X_train, X_seedset[selection_ranking, :]))\n",
        "    y_train = np.concatenate((y_train, y_seedset[selection_ranking]))\n",
        "\n",
        "    X_seedset = np.delete(X_seedset, selection_ranking, axis=0)\n",
        "    y_seedset = np.delete(y_seedset, selection_ranking, axis=0)\n",
        "\n",
        "    print('After selecting samples based on Samping methods on round',iterations+1,' :')\n",
        "    print('Train : ', X_train.shape, y_train.shape)\n",
        "    print('Seedset  : ', X_seedset.shape, y_seedset.shape)\n",
        "\n",
        "    y_train_bin = y_train.reshape((y_train.shape[0],))\n",
        "    bin_count = np.bincount(y_train_bin.astype('int64'))\n",
        "    unique = np.unique(y_train_bin.astype('int64'))\n",
        "    print ('Total number of samples :', bin_count, ' in each classes',unique )\n",
        "    print('-------Finish training anthor 1000 samples on round ',iterations+1,'----------------')\n",
        "    iterations += 1\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cifar Train: X=(50000, 32, 32, 3), y=(50000, 1)\n",
            "Cifar Test: X=(10000, 32, 32, 3), y=(10000, 1)\n",
            "Initial random chosen samples (2000,)\n",
            "initial train set: (2000, 32, 32, 3) (2000, 1) \n",
            "labels count: [182 211 216 211 201 197 182 211 197 192] [0 1 2 3 4 5 6 7 8 9]\n",
            "Train set size X : (2000, 32, 32, 3)\n",
            "(2000, 1)\n",
            "Seed set (Pool) : (48000, 32, 32, 3) (48000, 1)\n",
            "\n",
            "Model 1 training\n",
            "X_train : (1400, 32, 32, 3)\n",
            "y train : (1400, 1)\n",
            "X val : (600, 32, 32, 3)\n",
            "y val: (600, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvT3a7BXUA4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "recordmodel1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmaxRgO7FT9l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}